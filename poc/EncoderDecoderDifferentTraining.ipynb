{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7nF7stufLSM"
   },
   "source": [
    "Training an Encoder Decoer Model in different ways\n",
    "\n",
    "1. Masked LM - Used for Encoder-Decoder type models (t5,flan-t5 etc)\n",
    "2. Causal LM - Used for Decoder type models (gpt2,bloom,palm etc)\n",
    "3. Teacher Forcing - Can be used for both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYt7rOpcBKPH",
    "outputId": "e95ef75c-9e48-4a75-df14-200733f903b7"
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZhTHhdTDqVU"
   },
   "source": [
    " ## Masked LM/ denoising training\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/model_doc/t5#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6siPNPvIdd8r"
   },
   "outputs": [],
   "source": [
    "# utility class for denoised training, taken from hugging face library\n",
    "class FlaxDataCollatorForT5MLM:\n",
    "    \"\"\"\n",
    "    From https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_t5_mlm_flax.py\n",
    "    \"\"\"\n",
    "    def __init__(self,tokenizer,noise_density,mean_noise_span_length) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.noise_density = noise_density\n",
    "        self.mean_noise_span_length =mean_noise_span_length\n",
    "\n",
    "    def create_sentinel_ids(self, mask_indices):\n",
    "        \"\"\"\n",
    "        Sentinel ids creation given the indices that should be masked.\n",
    "        The start indices of each mask are replaced by the sentinel ids in increasing\n",
    "        order. Consecutive mask indices to be deleted are replaced with `-1`.\n",
    "        \"\"\"\n",
    "        start_indices = mask_indices - np.roll(mask_indices, 1, axis=-1) * mask_indices\n",
    "        start_indices[:, 0] = mask_indices[:, 0]\n",
    "\n",
    "        sentinel_ids = np.where(start_indices != 0, np.cumsum(start_indices, axis=-1), start_indices)\n",
    "        sentinel_ids = np.where(sentinel_ids != 0, (len(self.tokenizer) - sentinel_ids), 0)\n",
    "        sentinel_ids -= mask_indices - start_indices\n",
    "\n",
    "        return sentinel_ids\n",
    "\n",
    "    def filter_input_ids(self, input_ids, sentinel_ids):\n",
    "        \"\"\"\n",
    "        Puts sentinel mask on `input_ids` and fuse consecutive mask tokens into a single mask token by deleting.\n",
    "        This will reduce the sequence length from `expanded_inputs_length` to `input_length`.\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        input_ids_full = np.where(sentinel_ids != 0, sentinel_ids, input_ids)\n",
    "        # input_ids tokens and sentinel tokens are >= 0, tokens < 0 are\n",
    "        # masked tokens coming after sentinel tokens and should be removed\n",
    "        input_ids = input_ids_full[input_ids_full >= 0].reshape((batch_size, -1))\n",
    "        input_ids = np.concatenate(\n",
    "            [input_ids, np.full((batch_size, 1), self.tokenizer.eos_token_id, dtype=np.int32)], axis=-1\n",
    "        )\n",
    "        return input_ids\n",
    "\n",
    "    def random_spans_noise_mask(self, length):\n",
    "        \"\"\"This function is copy of `random_spans_helper <https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2682>`__ .\n",
    "        # with the correction of this https://github.com/huggingface/transformers/pull/22938/files\n",
    "        Noise mask consisting of random spans of noise tokens.\n",
    "        The number of noise tokens and the number of noise spans and non-noise spans\n",
    "        are determined deterministically as follows:\n",
    "        num_noise_tokens = round(length * noise_density)\n",
    "        num_nonnoise_spans = num_noise_spans = round(num_noise_tokens / mean_noise_span_length)\n",
    "        Spans alternate between non-noise and noise, beginning with non-noise.\n",
    "        Subject to the above restrictions, all masks are equally likely.\n",
    "        Args:\n",
    "            length: an int32 scalar (length of the incoming token sequence)\n",
    "            noise_density: a float - approximate density of output mask\n",
    "            mean_noise_span_length: a number\n",
    "        Returns:\n",
    "            a boolean tensor with shape [length]\n",
    "        \"\"\"\n",
    "\n",
    "        orig_length = length\n",
    "\n",
    "        num_noise_tokens = int(np.round(length * self.noise_density))\n",
    "        num_nonnoise_tokens = length - num_noise_tokens\n",
    "        # avoid degeneracy by ensuring positive numbers of noise and nonnoise tokens.\n",
    "        num_noise_tokens = min(max(num_noise_tokens, 1), length - 1)\n",
    "        # num_noise_tokens should be less than num_noise_tokens and num_nonnoise_tokens\n",
    "        num_noise_spans = int(np.round(min(num_noise_tokens, num_nonnoise_tokens) / self.mean_noise_span_length))\n",
    "\n",
    "        # avoid degeneracy by ensuring positive number of noise spans\n",
    "        num_noise_spans = max(num_noise_spans, 1)\n",
    "\n",
    "        # pick the lengths of the noise spans and the non-noise spans\n",
    "        def _random_segmentation(num_items, num_segments):\n",
    "            \"\"\"Partition a sequence of items randomly into non-empty segments.\n",
    "            Args:\n",
    "                num_items: an integer scalar > 0\n",
    "                num_segments: an integer scalar in [1, num_items]\n",
    "            Returns:\n",
    "                a Tensor with shape [num_segments] containing positive integers that add\n",
    "                up to num_items\n",
    "            \"\"\"\n",
    "            mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n",
    "            np.random.shuffle(mask_indices)\n",
    "            first_in_segment = np.pad(mask_indices, [[1, 0]])\n",
    "            segment_id = np.cumsum(first_in_segment)\n",
    "            # count length of sub segments assuming that list is sorted\n",
    "            _, segment_length = np.unique(segment_id, return_counts=True)\n",
    "            return segment_length\n",
    "\n",
    "        noise_span_lengths = _random_segmentation(num_noise_tokens, num_noise_spans)\n",
    "        nonnoise_span_lengths = _random_segmentation(num_nonnoise_tokens, num_noise_spans)\n",
    "\n",
    "        interleaved_span_lengths = np.reshape(\n",
    "            np.stack([nonnoise_span_lengths, noise_span_lengths], axis=1), [num_noise_spans * 2]\n",
    "        )\n",
    "        span_starts = np.cumsum(interleaved_span_lengths)[:-1]\n",
    "        span_start_indicator = np.zeros((length,), dtype=np.int8)\n",
    "        span_start_indicator[span_starts] = True\n",
    "        span_num = np.cumsum(span_start_indicator)\n",
    "        is_noise = np.equal(span_num % 2, 1)\n",
    "\n",
    "        return is_noise[:orig_length]\n",
    "\n",
    "\n",
    "def get_denoised(FlaxDataCollatorForT5MLM, tokenizer, prompt):\n",
    "    encoded = tokenizer(prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "    batch_size =1\n",
    "    input_length = encoded.input_ids.shape[1]\n",
    "    denoiser = FlaxDataCollatorForT5MLM(tokenizer,.55,1.5)\n",
    "    mask_indices = np.asarray([denoiser.random_spans_noise_mask(input_length) for i in range(batch_size)])\n",
    "    labels_mask = ~mask_indices\n",
    "    input_ids_sentinel = denoiser.create_sentinel_ids(mask_indices.astype(np.int8))\n",
    "    labels_sentinel = denoiser.create_sentinel_ids(labels_mask.astype(np.int8))\n",
    "    input_ids = denoiser.filter_input_ids(encoded.input_ids, input_ids_sentinel)\n",
    "    labels  =  denoiser.filter_input_ids(encoded.input_ids, labels_sentinel)\n",
    "    return labels,input_ids\n",
    "\n",
    "\n",
    "\n",
    "def print_token_id(tokenizer,token):\n",
    "  # Encode the token\n",
    "  encoded = tokenizer.encode(token)\n",
    "  # Print the id\n",
    "  print(token,encoded[0])\n",
    "  return encoded[0]\n",
    "\n",
    "def print_special_tokens(tokenizer):\n",
    "    # Special tokens and their ids\n",
    "    special_tokens = {}\n",
    "    for attr in tokenizer.special_tokens_map:\n",
    "        special_tokens[attr] = tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map[attr])\n",
    "\n",
    "    # Print special tokens\n",
    "    print(special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4pzIZJX5TkgU"
   },
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids, pad_token_id, eos_token_id):\n",
    "  \"\"\" Shift input ids one token to the right, and add pad token at the first position, and eos token to the last \"\"\"\n",
    "  # Create a larger tensor that includes space for the EOS token\n",
    "  shifted_input_ids = torch.zeros((input_ids.shape[0], input_ids.shape[1] + 1), dtype=input_ids.dtype)\n",
    "\n",
    "  # Shift input_ids one step to the right\n",
    "  shifted_input_ids[:, 1:] = input_ids\n",
    "\n",
    "  # Set the first token to the pad_token_id\n",
    "  shifted_input_ids[:, 0] = pad_token_id\n",
    "\n",
    "  # Set the last token to the eos_token_id\n",
    "  shifted_input_ids[:, -1] = eos_token_id\n",
    "\n",
    "  return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3A_klNwqdF8H",
    "outputId": "abcd58ac-f1ad-4ab5-b03f-fdb3409df160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 6]])\n"
     ]
    }
   ],
   "source": [
    " arr = np.array([[1, 2,3, 4,5]])\n",
    " arr = torch.tensor(arr)\n",
    " print(shift_tokens_right(arr,0,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LU5lvPDVNWd"
   },
   "source": [
    "## Using Masked LM for Seq to Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVM89j9bBcu6",
    "outputId": "c80f55af-9c5f-41dd-cf2d-763fc2977a17"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d59257f7824768a634f92c11cb1cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b1fbfc91dc48df90e91f0162e57a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6f3fadec8f41e9b56cc7063a1f5db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_tokenizer=32100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhilash22/venv/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = 't5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)# or T5Tokenizer\n",
    "len_tokenizer =len(tokenizer) # 32100 to get the sentinel ids\n",
    "print(f\"len_tokenizer={len_tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7BXCzRMB70L",
    "outputId": "09ce0738-fc52-4636-925d-86ff215cf4d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_prompt =tensor([[   37, 32099, 10681,    16, 32098,  2447,     1]])\n",
      "encoded_labels =tensor([[32099,  5295,  1782, 32098,     8, 32097,     1]])\n",
      "encoded_prompt.shape=encoded_labels.shape torch.Size([1, 7]) =torch.Size([1, 7])\n",
      "\n",
      "\n",
      "\n",
      "denoised input_ids decoded = The<extra_id_0> dog<extra_id_1> green park<extra_id_2></s>\n",
      "denoised labels decoded   = <extra_id_0> cute<extra_id_1> walks in the<extra_id_2></s></s>\n",
      "input_ids.shape (1, 8) labels.shape (1, 9)\n"
     ]
    }
   ],
   "source": [
    "# This below is what happens in the denoised training\n",
    "prompt = \"The <extra_id_0> walks in <extra_id_1> park\"\n",
    "encoded_prompt = tokenizer(prompt, truncation=False, padding=False, return_tensors=\"pt\").input_ids\n",
    "print(f\"encoded_prompt ={encoded_prompt}\")\n",
    "labels =\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\"\n",
    "encoded_labels = tokenizer(labels, truncation=False, padding=False, return_tensors=\"pt\").input_ids\n",
    "print(f\"encoded_labels ={encoded_labels}\")\n",
    "print(f\"encoded_prompt.shape=encoded_labels.shape {encoded_prompt.shape} ={encoded_labels.shape}\")\n",
    "\n",
    "# simulating the above\n",
    "\n",
    "print(\"\\n\"*2)\n",
    "\n",
    "prompt = \"The cute dog walks in the green park\"\n",
    "labels, input_ids = get_denoised(FlaxDataCollatorForT5MLM, tokenizer, prompt)\n",
    "print(f\"denoised input_ids decoded = {tokenizer.decode(*input_ids,skip_special_tokens=False)}\")\n",
    "print(f\"denoised labels decoded   = {tokenizer.decode(*labels,skip_special_tokens=False)}\")\n",
    "print(f\"input_ids.shape {input_ids.shape} labels.shape {labels.shape}\") # todo should this be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32099, 5295, 1782, 32098, 8, 32097, 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2 =\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\"\n",
    "tokenizer(labels2).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32099,  5295, 32098, 10681,    16, 32097,  2447,     1,     1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NQFdrw-YEJ8e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede7e4179ab94559ad77827a4800ab45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712892e958fa4f359dc679db033bf1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72263f679e84bea88ee2b79cacf849d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\") # or T5ForConditionalGeneration\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file, path='../NumEval - Task 3/'):\n",
    "    with open(os.path.join(path, file), 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "'''\n",
    "read train and dev files\n",
    "'''\n",
    "train_data = load_json('Train_Numerical_Reasoning.json')\n",
    "dev_data = load_json('Dev_Numerical_Reasoning.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sample, replace_token='mask', task='train'):\n",
    "    '''\n",
    "    teacher forcing only during training, hence reasoning prompt would be prepended only to the train samples\n",
    "    '''\n",
    "    \n",
    "    news = sample['news']\n",
    "    masked_headline = sample['masked headline']\n",
    "    calculation = sample['calculation']\n",
    "    ans = \" \" + str(sample['ans']) + \" \"\n",
    "    \n",
    "    if replace_token=='mask':\n",
    "        replace_token = \"<extra_id_0>\"\n",
    "        input_prompt = news + \" \" + masked_headline.replace('____', replace_token)\n",
    "    else:\n",
    "        input_prompt = news + \" \" + masked_headline.replace('____', ans)\n",
    "    \n",
    "    return input_prompt\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return tokenizer.encode_plus(sentence,\n",
    "                                 max_length=512,\n",
    "                                 padding='max_length',\n",
    "                                 truncation=\"only_first\",\n",
    "                                 return_tensors='pt',\n",
    "                                 return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21157it [00:00, 337171.77it/s]\n",
      "2572it [00:00, 314099.57it/s]\n"
     ]
    }
   ],
   "source": [
    "train_processed = []\n",
    "dev_processed = []\n",
    "\n",
    "for i, sample in tqdm(enumerate(train_data)):\n",
    "    if i==10575:\n",
    "        continue\n",
    "    else:\n",
    "        train_processed.append(process_data(sample))\n",
    "\n",
    "for i, sample in tqdm(enumerate(dev_data)):\n",
    "    dev_processed.append(process_data(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, prompts):\n",
    "        'Initialization'\n",
    "        self.prompts = prompts\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        return self.prompts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size':16,\n",
    "    'shuffle':False\n",
    "}\n",
    "\n",
    "train_set = NumDataset(train_processed)\n",
    "training_generator = torch.utils.data.DataLoader(train_set, **params)\n",
    "\n",
    "dev_set = NumDataset(dev_processed)\n",
    "dev_generator = torch.utils.data.DataLoader(dev_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.07789400219917297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:25,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.06955459713935852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:50,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.9799178242683411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:15,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.11499893665313721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [01:40,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.011922817677259445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:06,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.009716815315186977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [02:31,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.03928535804152489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [02:57,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0045724892988801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [03:25,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.026859100908041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [03:52,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.33264219760894775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [04:18,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.01629958674311638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "220it [04:45,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.6690669655799866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [05:11,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.06689233332872391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "260it [05:37,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.011732911691069603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [06:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.4229007959365845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [06:29,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.5053970813751221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "320it [06:55,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.016468189656734467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [07:21,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0010785376653075218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360it [07:47,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.012188335880637169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "380it [08:14,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.004730473272502422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [08:41,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.019742175936698914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "420it [09:08,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.003066584700718522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [09:35,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0064338697120547295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "460it [10:02,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0029338602907955647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [10:28,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.000575218815356493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [10:55,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.004621327854692936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [11:22,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.002053274307399988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "540it [11:49,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.007949749939143658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "560it [12:17,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.010626201517879963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [12:47,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.002431541681289673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [13:16,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.002975636161863804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "620it [13:46,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0036247691605240107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "640it [14:16,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.003343170741572976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "660it [14:47,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.002862487453967333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "680it [15:18,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0016629482852295041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700it [15:49,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.007615310605615377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "720it [16:20,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0026677215937525034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "740it [16:50,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0006694476469419897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "760it [17:19,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0009715721826069057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [17:48,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.001226431573741138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [18:17,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.001245873630978167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "820it [18:46,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.00022867444204166532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "840it [19:14,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0017261299071833491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "860it [19:42,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0009141184855252504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "880it [20:11,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0006275668856687844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900it [20:39,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.001120431930758059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "920it [21:07,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0007306902552954853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "940it [21:36,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.000612205418292433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "960it [22:04,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.000849278992973268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "980it [22:33,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0011348326224833727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [23:02,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.00032187538454309106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1020it [23:30,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0006287314463406801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1040it [23:58,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0003821659192908555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [24:26,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.00016275572124868631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1080it [24:55,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0006891119992360473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1100it [25:23,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.00037699550739489496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1120it [25:51,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0010686098830774426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1140it [26:19,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0001311562373302877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1160it [26:47,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0004070055147167295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [27:15,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0007928189006634057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [27:43,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.00038377640885300934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1220it [28:11,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0006537500885315239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240it [28:39,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0003150069678667933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1260it [29:07,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.00019131542649120092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1280it [29:35,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.00016814906848594546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1300it [30:03,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0005395199405029416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1320it [30:32,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 0.0002108645421685651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1323it [30:36,  1.39s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0003030502994079143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:28,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00018077527056448162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:56,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00024764114641584456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:24,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00023361039347946644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [01:53,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 8.824485848890617e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:22,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0004252936923876405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [02:51,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00016344586038030684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [03:19,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 9.261619561584666e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [03:47,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0008391097653657198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [04:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0004090219154022634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [04:44,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00033194359275512397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "220it [05:12,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00024051112995948642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [05:41,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 6.818146357545629e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "260it [06:08,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0001686817704467103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [06:37,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 4.097905184607953e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [07:05,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 7.71735722082667e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "320it [07:33,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 6.650095019722357e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [08:01,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00031529448460787535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360it [08:29,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 6.105688953539357e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "380it [08:57,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0002668470551725477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [09:26,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 4.166790677118115e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "420it [09:54,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 2.4264816602226347e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [10:22,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 3.705896597239189e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "460it [10:50,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 4.754846668220125e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [11:18,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 2.9907134376117028e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [11:47,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 6.367819878505543e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [12:15,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00011960459960391745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "540it [12:43,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 2.617196696519386e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "560it [13:11,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 2.2847654690849595e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [13:40,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00015735754277557135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [14:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00022422974871005863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "620it [14:36,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00013689971819985658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "640it [15:04,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 2.1072955860290676e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "660it [15:31,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 3.19998616760131e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "680it [15:59,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 5.469996904139407e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700it [16:27,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 1.7364440282108262e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "720it [16:54,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 8.243259799201041e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "740it [17:23,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 1.4874393855279777e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "760it [17:51,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0001179239188786596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [18:19,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 2.329816015844699e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [18:47,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 9.209606650983915e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "820it [19:15,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 3.560205368557945e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "840it [19:43,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0001031850406434387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "860it [20:10,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.000416235881857574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "880it [20:38,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00017554954683873802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900it [21:06,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 1.0702197869250085e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "920it [21:34,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0001513587194494903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "940it [22:02,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00018704681133385748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "960it [22:30,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0001174847930087708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "980it [23:08,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00012433278607204556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [23:36,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00013235994265414774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1020it [24:11,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00010571414895821363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1040it [24:39,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.0002294821315445006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [25:07,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 8.318039363075513e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1080it [25:35,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 7.80232367105782e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1100it [26:03,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 8.894873462850228e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1120it [26:31,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 7.205451311165234e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1140it [26:59,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 6.7286287048773374e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1160it [27:26,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 3.315221692901105e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [27:54,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 5.9069534472655505e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [28:29,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 6.056840720702894e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1220it [28:56,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 4.97595829074271e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240it [29:24,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 3.827763794106431e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1260it [30:08,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00016697643150109798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1280it [30:39,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 5.655773293256061e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1300it [31:25,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 6.778639362892136e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1320it [31:53,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 0.00020157432300038636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1323it [31:57,  1.45s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 4.88754449179396e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:28,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 0.00016037265595514327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:56,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 3.999896580353379e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:24,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 6.879684224259108e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [01:53,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 3.084736454184167e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:21,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.7337870303890668e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [02:49,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 3.888478750013746e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [03:17,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.4609251340734772e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [03:45,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.9243923563626595e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [04:13,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 4.931075091008097e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [04:41,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 9.277691424358636e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "220it [05:08,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.0370975107653067e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [05:36,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 0.0001217878088937141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "260it [06:04,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 3.023868521268014e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [06:32,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.2370419173967093e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [07:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.9642153347376734e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "320it [07:28,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 3.3378430543962168e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [07:56,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.6966818293440156e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360it [08:24,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.593396154930815e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "380it [08:52,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.455649544368498e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [09:20,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 3.0994272037787596e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "420it [09:48,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 0.00013523278175853193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [10:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 6.729603774147108e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "460it [10:44,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 9.711771417642012e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [11:12,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.1629351320152637e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [11:41,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.2278231224627234e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [12:09,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.64769605564652e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "540it [12:37,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.7815383418783313e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "560it [13:05,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 5.4343356168828905e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [13:33,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 4.808076027984498e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [14:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 4.463527875486761e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "620it [14:29,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.092777549478342e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "640it [14:57,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 4.664849257096648e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "660it [15:24,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.145727921742946e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "680it [15:52,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.154988058260642e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700it [16:20,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.0331243174732663e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "720it [16:48,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 8.529965271009132e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "740it [17:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.4066406947677024e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "760it [17:44,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 8.834630534693133e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [18:12,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 7.695527528994717e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [18:40,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 6.649170245509595e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "820it [19:08,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.237115040770732e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "840it [19:36,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.4569989161827834e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "860it [20:04,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 0.0001030268394970335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "880it [20:31,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 3.713897240231745e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900it [20:59,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.837117270042654e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "920it [21:27,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.4435892555629835e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "940it [21:55,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 3.6822177662543254e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "960it [22:23,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.6715539459255524e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "980it [22:51,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 8.504263678332791e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [23:19,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 7.45007855584845e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1020it [23:47,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 6.265057891141623e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1040it [24:15,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.529875928303227e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [24:43,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 3.283447585999966e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1080it [25:11,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 6.17232808508561e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1100it [25:39,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.6821713870740496e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1120it [26:07,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 4.927285317535279e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1140it [26:36,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 6.794829459977336e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1160it [27:04,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 5.404119292506948e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [27:32,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 6.27827103016898e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [28:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 4.1325652091472875e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1220it [28:28,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 6.563895294675604e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240it [28:56,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 2.8543203370645642e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1260it [29:24,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 5.1392016757745296e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1280it [29:51,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.5099806205398636e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1300it [30:19,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 4.265021743776742e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1320it [30:47,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.4066521544009447e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1323it [30:52,  1.40s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  Loss 5.5348868045257404e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:27,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  Loss 3.5232817481301026e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:55,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  Loss 1.3139351722202264e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:23,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  Loss 1.7219068695339956e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [01:51,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  Loss 4.6899418521206826e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [01:56,  1.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [55], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m denoised_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(labels)\n\u001b[1;32m      8\u001b[0m denoised_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoised_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoised_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoised_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ind \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1112\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     )\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:695\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    705\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:602\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    600\u001b[0m ):\n\u001b[1;32m    601\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 602\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    612\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:521\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# get query states\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m query_states \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[1;32m    524\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    525\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    526\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for ind, batch in tqdm(enumerate(training_generator)):\n",
    "        prompts = batch\n",
    "        \n",
    "        labels, input_ids = get_denoised(FlaxDataCollatorForT5MLM, tokenizer, prompt)\n",
    "        denoised_input_ids = torch.from_numpy(input_ids)\n",
    "        denoised_labels = torch.from_numpy(labels)\n",
    "        denoised_attention_mask = torch.ones(input_ids.shape)\n",
    "        \n",
    "        outputs = model(input_ids=denoised_input_ids,attention_mask=denoised_attention_mask,\n",
    "                      labels=denoised_labels)\n",
    "        loss = outputs.loss\n",
    "        if ind % 20 == 0:\n",
    "            print(f\"Epoch {epoch}  Loss {loss}\")\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21157it [00:00, 308200.34it/s]\n",
      "2572it [00:00, 319551.82it/s]\n"
     ]
    }
   ],
   "source": [
    "train_for_eval = []\n",
    "dev_for_eval = []\n",
    "for i, sample in tqdm(enumerate(train_data)):\n",
    "    if i==10575:\n",
    "        continue\n",
    "    else:\n",
    "        train_for_eval.append(process_data(sample))\n",
    "\n",
    "for i, sample in tqdm(enumerate(dev_data)):\n",
    "    dev_for_eval.append(process_data(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(Oct 7, 2014  12:40 PM CDT) As of Jan. 1, Walmart will no longer offer 30,000 of its employees health insurance. Bloomberg notes that's about 2% of its workforce. The move comes as a reaction to the company's rising health care costs as far more of its employees and their families enrolled in its health care plans than it had expected following the ObamaCare rollout. The AP reports those costs will surge $500 million this fiscal year, $170 million more than had been estimated. Those affected are employees who average fewer than 30 hours of work per week; the Wall Street Journal explains they were grandfathered in when Walmart in 2012 stopped offering insurance to new hires who didn't exceed the 30-hour threshold. A benefits expert says Walmart is actually late to the game in terms of cutting insurance to some part-time workers; Target, the Home Depot, and others have already done so. Meanwhile, Walmart's full time workers will see their premiums rise in 2015. Premiums for the basic plan, which 40% of its workforce is on, will increase 19% to $21.90 per pay period come Jan. 1. <extra_id_0>K Walmart Part-Timers to Lose Health Insurance\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_for_eval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training: (Oct 7, 2014  12:40 PM CDT) As of Jan. 1, Walmart will no longer offer 30,000 of its employees health insurance. Bloomberg notes that's about 2% of its workforce. The move comes as a reaction to the company's rising health care costs as far more of its employees and their families enrolled in its health care plans than it had expected following the ObamaCare rollout. The AP reports those costs will surge $500 million this fiscal year, $170 million more than had been estimated. Those affected are employees who average fewer than 30 hours of work per week; the Wall Street Journal explains they were grandfathered in when Walmart in 2012 stopped offering insurance to new hires who didn't exceed the 30-hour threshold. A benefits expert says Walmart is actually late to the game in terms of cutting insurance to some part-time workers; Target, the Home Depot, and others have already done so. Meanwhile, Walmart's full time workers will see their premiums rise in 2015. Premiums for the basic plan, which 40% of its workforce is on, will increase 19% to $21.90 per pay period come Jan. 1. <extra_id_0>K Walmart Part-Timers to Lose Health Insurance \n",
      "\n",
      "\n",
      "Prediction: 0 Comments\n"
     ]
    }
   ],
   "source": [
    "# After  training\n",
    "model.eval()\n",
    "test_prompt = train_for_eval[0]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=5)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"After Training: {test_prompt} \\n\\n\")\n",
    "print(f\"Prediction: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (Oct 29, 2013  8:15 AM CDT) Dax Shepard and Kristen Bell got married at the Beverly Hills courthouse, in a ceremony about as different from Kim Kardashian's last wedding extravaganza as it is possible to be. As Shepard revealed last night on Jimmy Kimmel Live, the whole thingincluding the fuel it took to get to the courthousecost $142.  It was just Kristen and I at this lonely courthouse,  he said, so friends showed up afterward with a cake reading, in icing,  The World's Worst Wedding.   How many people can say they threw the world's worst wedding?  Shepard asked. Dax Shepard: Wedding to Kristen Bell Cost $ 142  \n",
      "\n",
      "\n",
      "After Training: (Oct 29, 2013  8:15 AM CDT) Dax Shepard and Kristen Bell got married at the Beverly Hills courthouse, in a ceremony about as different from Kim Kardashian's last wedding extravaganza as it is possible to be. As Shepard revealed last night on Jimmy Kimmel Live, the whole thingincluding the fuel it took to get to the courthousecost $142.  It was just Kristen and I at this lonely courthouse,  he said, so friends showed up afterward with a cake reading, in icing,  The World's Worst Wedding.   How many people can say they threw the world's worst wedding?  Shepard asked. Dax Shepard: Wedding to Kristen Bell Cost $<extra_id_0> \n",
      "\n",
      "\n",
      "Prediction: 142?\n"
     ]
    }
   ],
   "source": [
    "# After  training\n",
    "model.eval()\n",
    "test_prompt = train_for_eval[1]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=125)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"Original: {train_processed[1]} \\n\\n\")\n",
    "print(f\"After Training: {test_prompt} \\n\\n\")\n",
    "print(f\"Prediction: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (Mar 6, 2016  10:50 AM) Nancy Reagan, the helpmate, backstage adviser, and fierce protector of Ronald Reagan in his journey from actor to presidentand finally during his 10-year battle with Alzheimer's diseasedied Sunday at the age of 94, reports the AP, via CBS News. The cause was congestive heart failure, notes ABC News. In addition to her famous campaign against drugs, the one-time actress promoted several causes while she was in the White House and even in the years after. She was a passionate advocate for lifting restrictions on stem cell research and promoting better treatment of America's veterans. But above all, Nancy Reagan was a fiercely devoted wife.  My life began with Ronnie,  she told Vanity Fair magazine in 1998. The first lady's public life had its share of controversy but also earned the respect of the nation, making Nancy Reagan one of America's most admired women in the 1980s and beyond. Anne Frances  Nancy  Robbins was born on July 6, 1921 in New York City to Kenneth Robbins, a car salesman, and Edith Luckett Robbins, an actress. She met Ronald Reagan in 1950, when he was president of the Screen Actors Guild and she was seeking help with a problem: Her name had been wrongly included on a published list of suspected communist sympathizers. They discussed it over dinner, and she later wrote that she realized on that first blind date  he was everything that I wanted.  They wed two years later, on March 4, 1952. She was thrust into the political life when her husband ran for California governor in 1966 and won. She found it a surprisingly rough business.  The movies were custard compared to politics,  she said. The couple had two children together, Patricia Ann and Ronald Prescott. Reagan will be buried next to her late husband at the Ronald Reagan Presidential Library in Simi Valley, California. The New York Times has a full obituary here. Nancy Reagan Dead at  94  \n",
      "\n",
      "\n",
      "After Training: (Mar 6, 2016  10:50 AM) Nancy Reagan, the helpmate, backstage adviser, and fierce protector of Ronald Reagan in his journey from actor to presidentand finally during his 10-year battle with Alzheimer's diseasedied Sunday at the age of 94, reports the AP, via CBS News. The cause was congestive heart failure, notes ABC News. In addition to her famous campaign against drugs, the one-time actress promoted several causes while she was in the White House and even in the years after. She was a passionate advocate for lifting restrictions on stem cell research and promoting better treatment of America's veterans. But above all, Nancy Reagan was a fiercely devoted wife.  My life began with Ronnie,  she told Vanity Fair magazine in 1998. The first lady's public life had its share of controversy but also earned the respect of the nation, making Nancy Reagan one of America's most admired women in the 1980s and beyond. Anne Frances  Nancy  Robbins was born on July 6, 1921 in New York City to Kenneth Robbins, a car salesman, and Edith Luckett Robbins, an actress. She met Ronald Reagan in 1950, when he was president of the Screen Actors Guild and she was seeking help with a problem: Her name had been wrongly included on a published list of suspected communist sympathizers. They discussed it over dinner, and she later wrote that she realized on that first blind date  he was everything that I wanted.  They wed two years later, on March 4, 1952. She was thrust into the political life when her husband ran for California governor in 1966 and won. She found it a surprisingly rough business.  The movies were custard compared to politics,  she said. The couple had two children together, Patricia Ann and Ronald Prescott. Reagan will be buried next to her late husband at the Ronald Reagan Presidential Library in Simi Valley, California. The New York Times has a full obituary here. Nancy Reagan Dead at <extra_id_0> \n",
      "\n",
      "\n",
      "Prediction: 94? Read More.\n"
     ]
    }
   ],
   "source": [
    "# After  training\n",
    "model.eval()\n",
    "test_prompt = train_for_eval[2]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=125)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"Original: {train_processed[2]} \\n\\n\")\n",
    "print(f\"After Training: {test_prompt} \\n\\n\")\n",
    "print(f\"Prediction: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (Aug 15, 2008  5:11 AM CDT) American Airlines faces FAA fines of more than $7 million for a series of safety and maintenance violations and for deficiencies in its drug and alcohol testing, the Wall Street Journal reports. In proposing one of its biggest fines ever, the FAA accuses American of knowingly flying planes that needed safety repairs, including one MD-80 that flew several times in 2007 with a faulty autopilot. American claims the violations were largely technical and plans to contest the  excessive  penalty.  We do not agree with the FAA's findings and characterizations of American's action in these cases,  said a spokesman. It's the latest of example of the FAA's growing aggressiveness on maintenance supervision, the Journal notes. American Airlines Faces $ 7 M Fine for Safety Violations \n",
      "\n",
      "\n",
      "After Training: (Aug 15, 2008  5:11 AM CDT) American Airlines faces FAA fines of more than $7 million for a series of safety and maintenance violations and for deficiencies in its drug and alcohol testing, the Wall Street Journal reports. In proposing one of its biggest fines ever, the FAA accuses American of knowingly flying planes that needed safety repairs, including one MD-80 that flew several times in 2007 with a faulty autopilot. American claims the violations were largely technical and plans to contest the  excessive  penalty.  We do not agree with the FAA's findings and characterizations of American's action in these cases,  said a spokesman. It's the latest of example of the FAA's growing aggressiveness on maintenance supervision, the Journal notes. American Airlines Faces $<extra_id_0>M Fine for Safety Violations \n",
      "\n",
      "\n",
      "Prediction: 7.5\n"
     ]
    }
   ],
   "source": [
    "# After  training\n",
    "model.eval()\n",
    "test_prompt = train_for_eval[3]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=3)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"Original: {train_processed[3]} \\n\\n\")\n",
    "print(f\"After Training: {test_prompt} \\n\\n\")\n",
    "print(f\"Prediction: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (Dec 24, 2014  11:19 AM) Turns out you won't even have to leave your house if you want to watch controversial Sony flick The Interview. The company has announced that, as of 1pm Eastern today, the Seth Rogen and James Franco movie about a plot to assassinate Kim Jong Un is available on YouTube, Google Play, Xbox, and possibly Sony's site SeetheInterview.com. It'll cost you $14.99 to buy or $5.99 to rent, Business Insider reports. The movie was initially pulled from release after the Sony hack and ensuing threats, but it will also be released in some theaters tomorrow after all. You Can Watch The Interview at  1 pm \n",
      "\n",
      "\n",
      "After Training: (Dec 24, 2014  11:19 AM) Turns out you won't even have to leave your house if you want to watch controversial Sony flick The Interview. The company has announced that, as of 1pm Eastern today, the Seth Rogen and James Franco movie about a plot to assassinate Kim Jong Un is available on YouTube, Google Play, Xbox, and possibly Sony's site SeetheInterview.com. It'll cost you $14.99 to buy or $5.99 to rent, Business Insider reports. The movie was initially pulled from release after the Sony hack and ensuing threats, but it will also be released in some theaters tomorrow after all. You Can Watch The Interview at <extra_id_0>pm \n",
      "\n",
      "\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "# After  training\n",
    "test_prompt = dev_for_eval[3]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=3)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"Original: {dev_processed[3]} \\n\\n\")\n",
    "print(f\"After Training: {test_prompt} \\n\\n\")\n",
    "print(f\"Prediction: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter(output, end_token='<extra_id_0>'):\n",
    "    # The first token is <unk> (inidex at 0) and the second token is <extra_id_0> (indexed at 32099)\n",
    "    _txt = tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    if end_token in _txt:\n",
    "        _end_token_index = _txt.index(end_token)\n",
    "        return _result_prefix + _txt[:_end_token_index] + _result_suffix\n",
    "    else:\n",
    "        return _result_prefix + _txt + _result_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"(Aug 15, 2008  5:11 AM CDT) American Airlines faces FAA fines of more than $7 million for a series of safety and maintenance violations and for deficiencies in its drug and alcohol testing, the Wall Street Journal reports. In proposing one of its biggest fines ever, the FAA accuses American of knowingly flying planes that needed safety repairs, including one MD-80 that flew several times in 2007 with a faulty autopilot. American claims the violations were largely technical and plans to contest the  excessive  penalty.  We do not agree with the FAA's findings and characterizations of American's action in these cases,  said a spokesman. It's the latest of example of the FAA's growing aggressiveness on maintenance supervision, the Journal notes. American Airlines Faces $7.5<extra_id_1>7.5M Fine for Safety Violations\"]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = train_for_eval[3]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=encoded.input_ids, \n",
    "                          num_beams=200, num_return_sequences=1,\n",
    "                          max_length=5)\n",
    "_0_index = test_prompt.index('<extra_id_0>')\n",
    "_result_prefix = test_prompt[:_0_index]\n",
    "_result_suffix = test_prompt[_0_index+12:] # 12 is the length of <extra_id_0>\n",
    "results = list(map(_filter, outputs))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"(Oct 7, 2014  12:40 PM CDT) As of Jan. 1, Walmart will no longer offer 30,000 of its employees health insurance. Bloomberg notes that's about 2% of its workforce. The move comes as a reaction to the company's rising health care costs as far more of its employees and their families enrolled in its health care plans than it had expected following the ObamaCare rollout. The AP reports those costs will surge $500 million this fiscal year, $170 million more than had been estimated. Those affected are employees who average fewer than 30 hours of work per week; the Wall Street Journal explains they were grandfathered in when Walmart in 2012 stopped offering insurance to new hires who didn't exceed the 30-hour threshold. A benefits expert says Walmart is actually late to the game in terms of cutting insurance to some part-time workers; Target, the Home Depot, and others have already done so. Meanwhile, Walmart's full time workers will see their premiums rise in 2015. Premiums for the basic plan, which 40% of its workforce is on, will increase 19% to $21.90 per pay period come Jan. 1. Related:<extra_id_1>K Walmart Part-Timers to Lose Health Insurance\"]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = train_for_eval[0]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=encoded.input_ids, \n",
    "                          num_beams=200, num_return_sequences=1,\n",
    "                          max_length=5)\n",
    "_0_index = test_prompt.index('<extra_id_0>')\n",
    "_result_prefix = test_prompt[:_0_index]\n",
    "_result_suffix = test_prompt[_0_index+12:] # 12 is the length of <extra_id_0>\n",
    "results = list(map(_filter, outputs))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (Apr 18, 2016  1:02 PM CDT) Ingrid Lyne, the Seattle mom allegedly murdered while on a date, left behind three daughtersand a GoFundMe campaign set up to help the girls has raised more than $222,000 so far, Us reports. A friend of the family set up the campaign, and says that all the money raised will go into a trust for the girls, who are ages 12, 10, and 7. Lyne's date was charged with her murder last week. $ 222 K Raised for Kids of Mom Dismembered on Date \n",
      "\n",
      "\n",
      "After Training: (Apr 18, 2016  1:02 PM CDT) Ingrid Lyne, the Seattle mom allegedly murdered while on a date, left behind three daughtersand a GoFundMe campaign set up to help the girls has raised more than $222,000 so far, Us reports. A friend of the family set up the campaign, and says that all the money raised will go into a trust for the girls, who are ages 12, 10, and 7. Lyne's date was charged with her murder last week. $<extra_id_0>K Raised for Kids of Mom Dismembered on Date \n",
      "\n",
      "\n",
      "Prediction: 2.2.\n"
     ]
    }
   ],
   "source": [
    "# After  training\n",
    "# model.eval()\n",
    "test_prompt = train_for_eval[4]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=5)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"Original: {train_processed[4]} \\n\\n\")\n",
    "print(f\"After Training: {test_prompt} \\n\\n\")\n",
    "print(f\"Prediction: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (Sep 19, 2014  12:58 PM CDT) Ezekiel Emanuel is a healthy 57-year-old in all respects, as his recent hike up Mount Kilimanjaro would suggest. Which is why it might be disconcerting to read his essay in the Atlantic laying out the reasons why he hopes to be dead in 18 years. To Emanuel, 75 is the right age at which to die.  I will have lived a complete life,  he writes. He will have seen his grandkids begin their own lives and will have  made whatever contributions, important or not, I am going to make.  And, with luck, the inevitable mental and physical declines of old age will not have set in yet. Emanuel rejects what he calls the  American immortal the concept that drives people to obsessively exercise, pop vitamins, do mental puzzles, etc., in the hope of cheating death. Yes, death is a loss, he writes.  But here is a simple truth that many of us seem to resist: living too long is also a loss. It renders many of us, if not disabled, then faltering and declining, a state that may not be worse than death but is nonetheless deprived.  Emanuel isn't talking about committing suicide on his 75th birthday, but he says his whole approach to health will change.  I wont actively end my life. But I wont try to prolong it, either.  No more cancer tests, for example, or even flu shots. Click to read the full, provocative essay, which ends with Emanuel retaining the right to change his mind and write another essay in 18 years arguing for a longer life.  That, after all, would mean still being creative after 75. I Don't Want to Live Past  75  \n",
      "\n",
      "\n",
      "After Training: (Sep 19, 2014  12:58 PM CDT) Ezekiel Emanuel is a healthy 57-year-old in all respects, as his recent hike up Mount Kilimanjaro would suggest. Which is why it might be disconcerting to read his essay in the Atlantic laying out the reasons why he hopes to be dead in 18 years. To Emanuel, 75 is the right age at which to die.  I will have lived a complete life,  he writes. He will have seen his grandkids begin their own lives and will have  made whatever contributions, important or not, I am going to make.  And, with luck, the inevitable mental and physical declines of old age will not have set in yet. Emanuel rejects what he calls the  American immortal the concept that drives people to obsessively exercise, pop vitamins, do mental puzzles, etc., in the hope of cheating death. Yes, death is a loss, he writes.  But here is a simple truth that many of us seem to resist: living too long is also a loss. It renders many of us, if not disabled, then faltering and declining, a state that may not be worse than death but is nonetheless deprived.  Emanuel isn't talking about committing suicide on his 75th birthday, but he says his whole approach to health will change.  I wont actively end my life. But I wont try to prolong it, either.  No more cancer tests, for example, or even flu shots. Click to read the full, provocative essay, which ends with Emanuel retaining the right to change his mind and write another essay in 18 years arguing for a longer life.  That, after all, would mean still being creative after 75. I Don't Want to Live Past <extra_id_0> \n",
      "\n",
      "\n",
      "Prediction: 75!\n"
     ]
    }
   ],
   "source": [
    "# After  training\n",
    "# model.eval()\n",
    "test_prompt = train_for_eval[5]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=5)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"Original: {train_processed[5]} \\n\\n\")\n",
    "print(f\"After Training: {test_prompt} \\n\\n\")\n",
    "print(f\"Prediction: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\d{1,3}(?:,\\d{3})+|\\d+[\\/\\.]{0,1}\\d+|\\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dev_processed = 2572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2572/2572 [23:27<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_gt = []\n",
    "num_gt = []\n",
    "print(f\"len of dev_processed = {len(dev_processed)}\")\n",
    "for i in tqdm(range(len(dev_processed))):\n",
    "    test_prompt = dev_for_eval[i]\n",
    "    encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "    test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=5)\n",
    "    test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    generated_num_list = pattern.findall(test_answer)\n",
    "    num_gt.append(dev_data[i]['ans'])\n",
    "    pred_gt += generated_num_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(f\"Original: {dev_processed[3]} \\n\\n\")\n",
    "    # print(f\"After Training: {test_prompt} \\n\\n\")\n",
    "    # print(f\"Prediction: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 't5MLM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2516"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_gt[:1000], num_gt[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4POirRLPH4E8",
    "outputId": "b6029fd3-0672-4c3d-ecd8-4fc5cc8e927f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training:'The  <extra_id_0> dog  walks in the <extra_id_2>'-->'cute park'\n"
     ]
    }
   ],
   "source": [
    "# After  training\n",
    "model.eval()\n",
    "test_prompt = \"The  <extra_id_0> dog  walks in the <extra_id_2>\"\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=125)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"After Training:'{test_prompt}'-->'{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL0tBixFWocM"
   },
   "source": [
    "## Masked LM for Generative Models  \n",
    "\n",
    "Let's try masked  LM on Decoder only model\n",
    "\n",
    "GPT-2 is a causal language model, designed to predict the next token in a sequence given the previous tokens. It's usually trained using a standard language modeling objective, without the use of denoising.\n",
    "\n",
    "As you can see from output, this sort of training is not effective for Generative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-5eHNRkXJpN"
   },
   "outputs": [],
   "source": [
    "  from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "  model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRhAXXPlXDPZ",
    "outputId": "bd45b984-d104-4343-c24d-77b5cc8d35c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denoised input_ids decoded = The<|endoftext|> dog gazed the green informants<|endoftext|>\n",
      "denoised labels decoded   = <|endoftext|> cute gazed walks in informants park<|endoftext|>\n",
      "input_ids.shape (1, 8) labels.shape (1, 8)\n",
      "Epoch 0  Loss 11.180167198181152\n",
      "Epoch 20  Loss 0.0005788219859823585\n",
      "Epoch 40  Loss 0.0019290262134745717\n",
      "Epoch 60  Loss 0.000539803528226912\n",
      "Epoch 80  Loss 5.3726726036984473e-05\n",
      "Epoch 99  Loss 0.8452759385108948\n"
     ]
    }
   ],
   "source": [
    "  prompt = \"The cute dog walks in the green park\"\n",
    "  labels, input_ids = get_denoised(FlaxDataCollatorForT5MLM, tokenizer, prompt)\n",
    "  print(f\"denoised input_ids decoded = {tokenizer.decode(*input_ids,skip_special_tokens=False)}\")\n",
    "  print(f\"denoised labels decoded   = {tokenizer.decode(*labels,skip_special_tokens=False)}\")\n",
    "  print(f\"input_ids.shape {input_ids.shape} labels.shape {labels.shape}\") # this should be equal for CausalLM models\n",
    "  denoised_input_ids = torch.from_numpy(input_ids)\n",
    "  denoised_labels = torch.from_numpy(labels)\n",
    "  denoised_attention_mask = torch.ones(input_ids.shape)\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(100):\n",
    "      outputs = model(input_ids=denoised_input_ids,attention_mask=denoised_attention_mask,\n",
    "                      labels=denoised_labels)\n",
    "      loss = outputs.loss\n",
    "      if epoch % 20 == 0:\n",
    "          print(f\"Epoch {epoch}  Loss {loss}\")\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "  print(f\"Epoch {epoch}  Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUkShQjHbrwp",
    "outputId": "d2aca30e-8be4-4801-b542-d79e054ea28f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training:'The  <extra_id_0> dog  walks in the <extra_id_2>'-->'The  <extra_id_0> dog  walks in the <extra_id_2> gazed in.\"'\n"
     ]
    }
   ],
   "source": [
    "  # After  training\n",
    "  model.eval()\n",
    "  test_prompt = \"The  <extra_id_0> dog  walks in the <extra_id_2>\"\n",
    "  encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "  test_output = model.generate(input_ids = encoded.input_ids,num_return_sequences=1,max_length=125)\n",
    "  test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "  print(f\"After Training:'{test_prompt}'-->'{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlZBahM1fJ7g"
   },
   "source": [
    "## Causal LM training - teacher forced for Sequence to Sequence Models\n",
    "\n",
    "We are using Teacher forcing for Language Modeling; Basically predicting the next word, from the previous workds.\n",
    "\n",
    "To force the labels to show the correct ground truth, we shift the label which is the ground truth to the right as shown below.\n",
    "\n",
    "This type of training can be used best for CausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ApoS4DBa5wU",
    "outputId": "bcae86f8-7cab-4a63-953b-819f97bdb8e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'eos_token': 1, 'unk_token': 2, 'pad_token': 0, 'additional_special_tokens': [32099, 32098, 32097, 32096, 32095, 32094, 32093, 32092, 32091, 32090, 32089, 32088, 32087, 32086, 32085, 32084, 32083, 32082, 32081, 32080, 32079, 32078, 32077, 32076, 32075, 32074, 32073, 32072, 32071, 32070, 32069, 32068, 32067, 32066, 32065, 32064, 32063, 32062, 32061, 32060, 32059, 32058, 32057, 32056, 32055, 32054, 32053, 32052, 32051, 32050, 32049, 32048, 32047, 32046, 32045, 32044, 32043, 32042, 32041, 32040, 32039, 32038, 32037, 32036, 32035, 32034, 32033, 32032, 32031, 32030, 32029, 32028, 32027, 32026, 32025, 32024, 32023, 32022, 32021, 32020, 32019, 32018, 32017, 32016, 32015, 32014, 32013, 32012, 32011, 32010, 32009, 32008, 32007, 32006, 32005, 32004, 32003, 32002, 32001, 32000]}\n"
     ]
    }
   ],
   "source": [
    "print_token_id(tokenizer,\"<\\s>\" )\n",
    "print_special_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KE6O6Gc-KEoD"
   },
   "outputs": [],
   "source": [
    "model_name =\"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdmWqOsjL2zr",
    "outputId": "8c3634f0-0074-448a-da35-652e91a71f7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher_forced input_ids    = tensor([   37,  5295,  1782, 10681,    16,     8,  1442,  2447,     1])\n",
      "teacher_forced input_ids decoded = The cute dog walks in the green park</s>\n",
      "teacher_forced labels    = tensor([    0,    37,  5295,  1782, 10681,    16,     8,  1442,  2447,     1])\n",
      "teacher_forced labels decoded   = <pad> The cute dog walks in the green park</s>\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"The cute dog walks in the green park\"\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=True, return_tensors=\"pt\")\n",
    "label_input_ids = shift_tokens_right(encoded.input_ids,model.config.pad_token_id,model.config.eos_token_id)\n",
    "\n",
    "print(f\"teacher_forced input_ids    = {(encoded.input_ids.squeeze())}\")\n",
    "print(f\"teacher_forced input_ids decoded = {tokenizer.decode(encoded.input_ids.squeeze(),skip_special_tokens=False)}\")# .squeeze() as it takes a batch\n",
    "print(f\"teacher_forced labels    = {(label_input_ids.squeeze())}\")\n",
    "print(f\"teacher_forced labels decoded   = {tokenizer.decode(label_input_ids.squeeze(),skip_special_tokens=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNJGqSSJDx8L",
    "outputId": "90e0327e-11fa-4897-bbee-9602545ceff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 2.868603229522705\n",
      "Epoch 40  Loss 0.622085690498352\n",
      "Epoch 80  Loss 0.21471920609474182\n",
      "Epoch 120  Loss 0.19337992370128632\n",
      "Epoch 160  Loss 0.2608181834220886\n",
      "Epoch 199  Loss 0.4793122708797455\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(200):\n",
    "    outputs = model(input_ids=encoded.input_ids,attention_mask=encoded.attention_mask,\n",
    "                    labels=label_input_ids)\n",
    "    loss = outputs.loss\n",
    "    if epoch % 40 == 0:\n",
    "        print(f\"Epoch {epoch}  Loss {loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(f\"Epoch {epoch}  Loss {loss}\")\n",
    "  #-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHzZh8OXZgGi",
    "outputId": "3cc506b7-1de9-411f-c3a3-7e150a30c61d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training:'The cute dog walks in the'-->'The cute dog walks in the green park'\n"
     ]
    }
   ],
   "source": [
    "  model.eval()\n",
    "  test_prompt = \"The cute dog walks in the\"\n",
    "  encoded = tokenizer(test_prompt, truncation=False, padding=True, return_tensors=\"pt\")\n",
    "  test_output = model.generate(input_ids = encoded.input_ids,max_new_tokens=125)\n",
    "  test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "  print(f\"After Training:'{test_prompt}'-->'{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0AyeQ_yZVVi"
   },
   "source": [
    "## Causal LM training -Teacher forced for Generative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azNE7rVseW8O",
    "outputId": "5a3299f0-1780-4db5-c83f-4b21adc01eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 50256\n",
      "[PAD] 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(model.config.pad_token_id,model.config.eos_token_id)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# GPT does not have the pad token set, so we add a new token\n",
    "pad_token_id = print_token_id(tokenizer,\"[PAD]\")\n",
    "tokenizer.pad_token = pad_token_id\n",
    "# and update the model with the token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "#model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQJs7srR8C9v",
    "outputId": "332d3e42-9214-4225-aab9-4422892a444a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher_forced input_ids decoded = The cute dog walks in the green park\n",
      "teacher_forced labels decoded   = [PAD]The cute dog walks in the green<|endoftext|>\n",
      "teacher_forced input_ids decoded = The cute dog walks in the green park<|endoftext|>\n",
      "torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"The cute dog walks in the green park\"\n",
    "\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=True, return_tensors=\"pt\")\n",
    "#from torch.nn.functional import pad\n",
    "# Pad the input_ids tensor on the right (at the end)\n",
    "#encoded.input_ids = pad(encoded.input_ids, (0, 1), value=tokenizer.pad_token_id)\n",
    "labels = shift_tokens_right(encoded.input_ids,pad_token_id,tokenizer.eos_token_id)\n",
    "\n",
    "print(f\"teacher_forced input_ids decoded = {tokenizer.decode(encoded.input_ids.squeeze(),skip_special_tokens=False)}\")# .squeeze() as it takes a batch\n",
    "print(f\"teacher_forced labels decoded   = {tokenizer.decode(labels.squeeze(),skip_special_tokens=False)}\")\n",
    "\n",
    "# we need the sizes to match\n",
    "\n",
    "# add a pad token to input ids to match the size\n",
    "new_token = torch.tensor([tokenizer.eos_token_id])\n",
    "new_token = new_token.view(1, -1)\n",
    "# Append the new token\n",
    "encoded.input_ids = torch.cat((encoded.input_ids, new_token),dim=1)\n",
    "print(f\"teacher_forced input_ids decoded = {tokenizer.decode(encoded.input_ids.squeeze(),skip_special_tokens=False)}\")#\n",
    "\n",
    "assert encoded.input_ids.size() == labels.size()\n",
    "\n",
    "attention_mask = torch.ones(encoded.input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "\n",
    "# Note that training with the above the loss do not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzRyrMFgK6CH",
    "outputId": "d97697e2-7d32-45f1-e0fa-c84819f3a47b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 7.023041248321533\n",
      "Epoch 9  Loss 8.018503189086914\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(10):\n",
    "    outputs = model(input_ids=encoded.input_ids,attention_mask=attention_mask,\n",
    "                    labels=labels)\n",
    "    loss = outputs.loss\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}  Loss {loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(f\"Epoch {epoch}  Loss {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_yBZCQvnLCC"
   },
   "outputs": [],
   "source": [
    "# Genertaive models need the input shape and target shape to be exact; I guess the shifting cell to right is automatically happening here\n",
    "# yes it is - https://discuss.huggingface.co/t/shifting-ids-to-the-right-when-training-gpt-2-on-text-generation/5308/2\n",
    "# no need for things in above cell\n",
    "\n",
    "# so lets try without shifting right; in the assumption that it is already taken care by the model\n",
    " #Note there is no padding for this tokenizeer-see above on how to set pad token for the tokenizer and model\n",
    "\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,GPT2LMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\") #GPT2LMHeadModel\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89GaBnpXiAj8",
    "outputId": "aba5a997-a9e0-4bdf-f59f-d9e7c22e8b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 5.569321155548096\n",
      "Epoch 20  Loss 1.2724746465682983\n",
      "Epoch 40  Loss 0.00023454656184185296\n",
      "Epoch 49  Loss 8.242394869739655e-06\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"The cute dog walks in the green park\"\n",
    "#test_prompt = [\"The cute\", \"cute dog\", \"dog walks\", \"walks in\" ,\"in the\", \"the green\", \"green park\"]\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding='longest', return_tensors=\"pt\") #\n",
    "label_input_ids = encoded.input_ids\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    outputs = model(input_ids=encoded.input_ids,attention_mask=encoded.attention_mask,\n",
    "                    labels=label_input_ids)\n",
    "    loss = outputs.loss\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}  Loss {loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(f\"Epoch {epoch}  Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hs5msm1DiomU",
    "outputId": "ce73ee42-a67d-4843-8a20-4a7d89339ed0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training:'The'-->'The cute dog walks in the green park park park park park green park park park in the green park park park park in the green'\n",
      "After Training:'Where does the cute dog walk'-->'Where does the cute dog walk in the green park park park park green park park park park in the green park park park park park in the green park park'\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_prompt = \"The\"\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=True, return_tensors=\"pt\")\n",
    "test_output = model.generate(encoded.input_ids,max_new_tokens=25)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"After Training:'{test_prompt}'-->'{test_answer}'\")\n",
    "test_prompt = \"Where does the cute dog walk\"\n",
    "encoded = tokenizer(test_prompt, truncation=False, padding=True, return_tensors=\"pt\")\n",
    "test_output = model.generate(encoded.input_ids,max_new_tokens=25)\n",
    "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "print(f\"After Training:'{test_prompt}'-->'{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Y1USYWdZ_D2"
   },
   "source": [
    "##  Teacher Forcing for Tasks\n",
    "\n",
    "The target here is not the input ids, but the labels.\n",
    "\n",
    "The label  is the ground truth (actual next word/sequence).\n",
    "\n",
    " During the forward pass, the model makes a prediction, and the difference\n",
    "  between the prediction and this ground truth is calculated to compute the loss.\n",
    "\n",
    "However here the training can be used for any arbitary tasks, like translation or QA etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "c5d481968a4f4dcabc0ab784a8901fe5",
      "1bf48e12aaa842628a6485183c77ee1c",
      "87ecbbca0f364431b370c64ff837cb52",
      "1602722c1a324f53b0b32893c19dec24",
      "469b81bc15024b19b49f80635a488d43",
      "2340f5bde16c436b98fb31040ebe8ac3",
      "ebcddc2826ef4bf5b86973f968d4309b",
      "393918e537b74cc49b90fd1e9c8928e0",
      "8135c65684cc45d692130b2a5e2384f8",
      "fd7689644ffd4091b8ea814fb360308f",
      "a7006bed9ead49ca8f77a20914551733",
      "fb098bb73d9c419588beedaba27243d0",
      "caab64a630144ebbbc05881e7a4a55a2",
      "f593eb0e5fcc4f0fab885e2bf79fbff4",
      "f3a836afdce34f6791d88db010815996",
      "ff31766975954b41af9a879c9de75389",
      "9c3ac9535cfb4cf088d797015e88d778",
      "3c019c41665f43e7a69940adf74250c7",
      "4431cb9feab546d5a44c6ff9dd92a6fe",
      "213a6320148f46ba8ffcdc425cd75bf1",
      "33f81372211e411eb843d9370b08d8d4",
      "011f9cced7e649428ed027a2ae001635",
      "a534a1aed6ad4f06afdf443f1d9e6aeb",
      "e36e810e4db84c18a195ef7c30e709a0",
      "52dbe55efcac4aa7add9faf135570390",
      "bb72ccf7ee36401597cad01b11c41caa",
      "4315517694784c08a3a7905814f77491",
      "6791748eeb33458cbe15fd91c17dd8a9",
      "29405f91b4d3430485f552e028fb03a9",
      "4cff9d7e3cbb43058574448a93f397e5",
      "29265864199f42e1bca517d49a05edd5",
      "42cfd25b02824d4ea4a54728a76f0258",
      "cfc63cd04cad4ae488f568db28433b5f",
      "2005c22d3b914843afbe60cf9ff5f516",
      "81fb850f99534f4192d2f5da05b8bb71",
      "9037f49096624c68918fedcfa02195a0",
      "a637b128f6114628a02cb3f09db39b07",
      "d5cf663eee124d46ba67ee2c4b7fdec3",
      "b0ccb22eefa0498199cb85270c7cb67b",
      "a5b70d352c5e48689459ee58fac27558",
      "e7c44c5fc1fe4737ac2fec84a7ac07f5",
      "92225f1eb9594642a9be771bcacd1ae1",
      "a73590f6a33f45f1baafe6e2611422be",
      "dfbc9031f9784e1e99eae563fd70b3fc",
      "a979906b0faf4e9fb95f8a2a730dcd71",
      "f2135e7031464a46b09bd40714fe60c6",
      "d635b06275fa491e8490627a9be60bf5",
      "bab1714fd69f4103abc3dad80e44c4a4",
      "1aafbc83798444c9a7896c7877e0cee2",
      "9830bb8e0c5e4514a8db9586e8fb29d9",
      "1f3c65359d20476f9f654a2dfa9d4f16",
      "833c402456be4440bdb3b6ff7832a016",
      "ff634fc9ef48439ea37bbd7378f38333",
      "6579f3e50ad54cf28e338be165b7d545",
      "e3be7e5d25fb4a74ae096fc1dcede1ff"
     ]
    },
    "id": "BVxHifpLaJFp",
    "outputId": "a0a7d72d-9fca-4308-89d3-9d912035beb1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d481968a4f4dcabc0ab784a8901fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb098bb73d9c419588beedaba27243d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a534a1aed6ad4f06afdf443f1d9e6aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2005c22d3b914843afbe60cf9ff5f516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a979906b0faf4e9fb95f8a2a730dcd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name =\"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zKW2CnYKPnK",
    "outputId": "80d6bfaa-c0d6-4ab0-c83f-469e60315106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 4.423830986022949\n",
      "Epoch 20  Loss 1.126334309577942\n",
      "Epoch 40  Loss 0.24285714328289032\n",
      "Epoch 49  Loss 0.27664294838905334\n"
     ]
    }
   ],
   "source": [
    "  test_prompt = \"Q:Where does the cute dog walk\"\n",
    "  label_prompt = \"A:In the green park\"\n",
    "  encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "  label = tokenizer(label_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "  model.train()\n",
    "  for epoch in range(50):\n",
    "      outputs = model(input_ids=encoded.input_ids,attention_mask=encoded.attention_mask,\n",
    "                      labels=label.input_ids)\n",
    "      loss = outputs.loss\n",
    "      if epoch % 20 == 0:\n",
    "          print(f\"Epoch {epoch}  Loss {loss}\")\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "  print(f\"Epoch {epoch}  Loss {loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdAxVu0bZdoh",
    "outputId": "5fbc193f-fc18-4330-a81b-cfcf827a9571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training:'Q:Where does the cute dog walk'-->'A:In the green park'\n"
     ]
    }
   ],
   "source": [
    "  #-------------------------------------------------------------\n",
    "  # After  training\n",
    "  model.eval()\n",
    "  test_prompt = \"Q:Where does the cute dog walk\"\n",
    "  encoded = tokenizer(test_prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "  test_output = model.generate(input_ids = encoded.input_ids,max_length=125)\n",
    "  test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "  print(f\"After Training:'{test_prompt}'-->'{test_answer}'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "011f9cced7e649428ed027a2ae001635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1602722c1a324f53b0b32893c19dec24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd7689644ffd4091b8ea814fb360308f",
      "placeholder": "",
      "style": "IPY_MODEL_a7006bed9ead49ca8f77a20914551733",
      "value": " 1.21k/1.21k [00:00&lt;00:00, 11.3kB/s]"
     }
    },
    "1aafbc83798444c9a7896c7877e0cee2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bf48e12aaa842628a6485183c77ee1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2340f5bde16c436b98fb31040ebe8ac3",
      "placeholder": "",
      "style": "IPY_MODEL_ebcddc2826ef4bf5b86973f968d4309b",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "1f3c65359d20476f9f654a2dfa9d4f16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2005c22d3b914843afbe60cf9ff5f516": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81fb850f99534f4192d2f5da05b8bb71",
       "IPY_MODEL_9037f49096624c68918fedcfa02195a0",
       "IPY_MODEL_a637b128f6114628a02cb3f09db39b07"
      ],
      "layout": "IPY_MODEL_d5cf663eee124d46ba67ee2c4b7fdec3"
     }
    },
    "213a6320148f46ba8ffcdc425cd75bf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2340f5bde16c436b98fb31040ebe8ac3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29265864199f42e1bca517d49a05edd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "29405f91b4d3430485f552e028fb03a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33f81372211e411eb843d9370b08d8d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "393918e537b74cc49b90fd1e9c8928e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c019c41665f43e7a69940adf74250c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42cfd25b02824d4ea4a54728a76f0258": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4315517694784c08a3a7905814f77491": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4431cb9feab546d5a44c6ff9dd92a6fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "469b81bc15024b19b49f80635a488d43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cff9d7e3cbb43058574448a93f397e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52dbe55efcac4aa7add9faf135570390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cff9d7e3cbb43058574448a93f397e5",
      "max": 147,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_29265864199f42e1bca517d49a05edd5",
      "value": 147
     }
    },
    "6579f3e50ad54cf28e338be165b7d545": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6791748eeb33458cbe15fd91c17dd8a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8135c65684cc45d692130b2a5e2384f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "81fb850f99534f4192d2f5da05b8bb71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0ccb22eefa0498199cb85270c7cb67b",
      "placeholder": "",
      "style": "IPY_MODEL_a5b70d352c5e48689459ee58fac27558",
      "value": "Downloading ()ve/main/spiece.model: 100%"
     }
    },
    "833c402456be4440bdb3b6ff7832a016": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87ecbbca0f364431b370c64ff837cb52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_393918e537b74cc49b90fd1e9c8928e0",
      "max": 1206,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8135c65684cc45d692130b2a5e2384f8",
      "value": 1206
     }
    },
    "9037f49096624c68918fedcfa02195a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7c44c5fc1fe4737ac2fec84a7ac07f5",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92225f1eb9594642a9be771bcacd1ae1",
      "value": 791656
     }
    },
    "92225f1eb9594642a9be771bcacd1ae1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9830bb8e0c5e4514a8db9586e8fb29d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c3ac9535cfb4cf088d797015e88d778": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a534a1aed6ad4f06afdf443f1d9e6aeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e36e810e4db84c18a195ef7c30e709a0",
       "IPY_MODEL_52dbe55efcac4aa7add9faf135570390",
       "IPY_MODEL_bb72ccf7ee36401597cad01b11c41caa"
      ],
      "layout": "IPY_MODEL_4315517694784c08a3a7905814f77491"
     }
    },
    "a5b70d352c5e48689459ee58fac27558": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a637b128f6114628a02cb3f09db39b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a73590f6a33f45f1baafe6e2611422be",
      "placeholder": "",
      "style": "IPY_MODEL_dfbc9031f9784e1e99eae563fd70b3fc",
      "value": " 792k/792k [00:00&lt;00:00, 4.39MB/s]"
     }
    },
    "a7006bed9ead49ca8f77a20914551733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a73590f6a33f45f1baafe6e2611422be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a979906b0faf4e9fb95f8a2a730dcd71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2135e7031464a46b09bd40714fe60c6",
       "IPY_MODEL_d635b06275fa491e8490627a9be60bf5",
       "IPY_MODEL_bab1714fd69f4103abc3dad80e44c4a4"
      ],
      "layout": "IPY_MODEL_1aafbc83798444c9a7896c7877e0cee2"
     }
    },
    "b0ccb22eefa0498199cb85270c7cb67b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bab1714fd69f4103abc3dad80e44c4a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6579f3e50ad54cf28e338be165b7d545",
      "placeholder": "",
      "style": "IPY_MODEL_e3be7e5d25fb4a74ae096fc1dcede1ff",
      "value": " 2.32k/2.32k [00:00&lt;00:00, 30.5kB/s]"
     }
    },
    "bb72ccf7ee36401597cad01b11c41caa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42cfd25b02824d4ea4a54728a76f0258",
      "placeholder": "",
      "style": "IPY_MODEL_cfc63cd04cad4ae488f568db28433b5f",
      "value": " 147/147 [00:00&lt;00:00, 2.37kB/s]"
     }
    },
    "c5d481968a4f4dcabc0ab784a8901fe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1bf48e12aaa842628a6485183c77ee1c",
       "IPY_MODEL_87ecbbca0f364431b370c64ff837cb52",
       "IPY_MODEL_1602722c1a324f53b0b32893c19dec24"
      ],
      "layout": "IPY_MODEL_469b81bc15024b19b49f80635a488d43"
     }
    },
    "caab64a630144ebbbc05881e7a4a55a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c3ac9535cfb4cf088d797015e88d778",
      "placeholder": "",
      "style": "IPY_MODEL_3c019c41665f43e7a69940adf74250c7",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "cfc63cd04cad4ae488f568db28433b5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5cf663eee124d46ba67ee2c4b7fdec3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d635b06275fa491e8490627a9be60bf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_833c402456be4440bdb3b6ff7832a016",
      "max": 2324,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ff634fc9ef48439ea37bbd7378f38333",
      "value": 2324
     }
    },
    "dfbc9031f9784e1e99eae563fd70b3fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e36e810e4db84c18a195ef7c30e709a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6791748eeb33458cbe15fd91c17dd8a9",
      "placeholder": "",
      "style": "IPY_MODEL_29405f91b4d3430485f552e028fb03a9",
      "value": "Downloading ()neration_config.json: 100%"
     }
    },
    "e3be7e5d25fb4a74ae096fc1dcede1ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e7c44c5fc1fe4737ac2fec84a7ac07f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebcddc2826ef4bf5b86973f968d4309b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f2135e7031464a46b09bd40714fe60c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9830bb8e0c5e4514a8db9586e8fb29d9",
      "placeholder": "",
      "style": "IPY_MODEL_1f3c65359d20476f9f654a2dfa9d4f16",
      "value": "Downloading ()okenizer_config.json: 100%"
     }
    },
    "f3a836afdce34f6791d88db010815996": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33f81372211e411eb843d9370b08d8d4",
      "placeholder": "",
      "style": "IPY_MODEL_011f9cced7e649428ed027a2ae001635",
      "value": " 242M/242M [00:02&lt;00:00, 94.1MB/s]"
     }
    },
    "f593eb0e5fcc4f0fab885e2bf79fbff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4431cb9feab546d5a44c6ff9dd92a6fe",
      "max": 242043056,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_213a6320148f46ba8ffcdc425cd75bf1",
      "value": 242043056
     }
    },
    "fb098bb73d9c419588beedaba27243d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_caab64a630144ebbbc05881e7a4a55a2",
       "IPY_MODEL_f593eb0e5fcc4f0fab885e2bf79fbff4",
       "IPY_MODEL_f3a836afdce34f6791d88db010815996"
      ],
      "layout": "IPY_MODEL_ff31766975954b41af9a879c9de75389"
     }
    },
    "fd7689644ffd4091b8ea814fb360308f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff31766975954b41af9a879c9de75389": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff634fc9ef48439ea37bbd7378f38333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
