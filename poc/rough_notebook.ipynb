{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546206e8-f5d4-41d6-aa65-0f5d67d83440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertForMaskedLM, BertModel, BertConfig,BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba81199-b191-4963-8948-afbc1c975349",
   "metadata": {},
   "source": [
    "## 0. Initialize params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec59a53-2456-40e6-ac83-226235427e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gpu/cpu\n",
    "'''\n",
    "cuda_index=2\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:'+str(cuda_index))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "'''\n",
    "modeling\n",
    "'''\n",
    "\n",
    "batch_size = 64\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16e044-0e45-4836-b2e1-3f682f4d561c",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0f459a-9f5c-4bd8-b008-0a2096170edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path, file):\n",
    "    with open(os.path.join(path, file), 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f4750d-b37d-4a82-9567-f9d44c9951f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'xlm-roberta-base',\n",
       " 'train_file': 'Train_Numerical_Reasoning.json',\n",
       " 'dev_file': 'Dev_Numerical_Reasoning.json',\n",
       " 'path': '../NumEval - Task 3/'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "read config file\n",
    "'''\n",
    "config = load_json('./', 'config.json')\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "363354d4-9385-49ca-b408-eae22034783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=config['path']\n",
    "train_file=config['train_file']\n",
    "dev_file=config['dev_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8ba3e2-08d8-4a5e-b86d-523733e80bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read train and dev files\n",
    "'''\n",
    "train_data = load_json(path, train_file)\n",
    "dev_data = load_json(path, dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538fb3f9-0fa9-4ffc-bce9-f03edd3867cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news': \"(Oct 7, 2014  12:40 PM CDT) As of Jan. 1, Walmart will no longer offer 30,000 of its employees health insurance. Bloomberg notes that's about 2% of its workforce. The move comes as a reaction to the company's rising health care costs as far more of its employees and their families enrolled in its health care plans than it had expected following the ObamaCare rollout. The AP reports those costs will surge $500 million this fiscal year, $170 million more than had been estimated. Those affected are employees who average fewer than 30 hours of work per week; the Wall Street Journal explains they were grandfathered in when Walmart in 2012 stopped offering insurance to new hires who didn't exceed the 30-hour threshold. A benefits expert says Walmart is actually late to the game in terms of cutting insurance to some part-time workers; Target, the Home Depot, and others have already done so. Meanwhile, Walmart's full time workers will see their premiums rise in 2015. Premiums for the basic plan, which 40% of its workforce is on, will increase 19% to $21.90 per pay period come Jan. 1.\",\n",
       " 'masked headline': '____K Walmart Part-Timers to Lose Health Insurance',\n",
       " 'calculation': 'Paraphrase(30,000,K)',\n",
       " 'ans': '30'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b22ff88-697a-43ab-9a00-3ae929401e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news': \"(Oct 29, 2013  8:15 AM CDT) Dax Shepard and Kristen Bell got married at the Beverly Hills courthouse, in a ceremony about as different from Kim Kardashian's last wedding extravaganza as it is possible to be. As Shepard revealed last night on Jimmy Kimmel Live, the whole thing—including the fuel it took to get to the courthouse—cost $142.  It was just Kristen and I at this lonely courthouse,  he said, so friends showed up afterward with a cake reading, in icing,  The World's Worst Wedding.   How many people can say they threw the world's worst wedding?  Shepard asked.\",\n",
       " 'masked headline': 'Dax Shepard: Wedding to Kristen Bell Cost $____',\n",
       " 'calculation': 'Copy(142)',\n",
       " 'ans': '142'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3224edb0-6ba6-4a79-b318-c448e7f54f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt(seqs):\n",
    "    \n",
    "    target = []\n",
    "    number_type = []\n",
    "    number_gt = []\n",
    "    \n",
    "    for ind, data in tqdm(enumerate(seqs)):\n",
    "        ans = str(data['ans'])\n",
    "        numtype = data['calculation']\n",
    "        stmt = data['masked headline'].replace('____', ans)\n",
    "        \n",
    "        if \"copy\" in numtype.lower():\n",
    "            calc = 1\n",
    "        else:\n",
    "            calc = 0\n",
    "            \n",
    "        target.append(stmt)\n",
    "        number_gt.append(ans)\n",
    "        number_type.append(calc)\n",
    "    return target, number_type, number_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d599293-fcc9-412b-a4a4-e4e8ed84e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21157it [00:00, 676358.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train_target, train_number_type, train_number_gt = create_gt(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02aee075-83d0-4e81-a940-e306f8a6e9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2572it [00:00, 513018.35it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_target, dev_number_type, dev_number_gt = create_gt(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dbf1a58-56eb-46c8-ba0f-20cb734884af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "target_file_path = 'target.txt'\n",
    "type_path = 'number_type.txt'\n",
    "gt_path = 'number_gt.txt'\n",
    "\n",
    "def write(tgt, numtype, numgt, trainval):    \n",
    "    # Open the file in write mode\n",
    "    with open(trainval+'_target.txt', 'w') as file:\n",
    "        # Write each element of the list to a new line\n",
    "        for item in tgt:\n",
    "            file.write(f\"{item}\\n\")\n",
    "    \n",
    "    with open(trainval+'_number_type.txt', 'w') as file:\n",
    "        # Write each element of the list to a new line\n",
    "        for item in numtype:\n",
    "            file.write(f\"{item}\\n\")\n",
    "    \n",
    "    with open(trainval+'_number_gt.txt', 'w') as file:\n",
    "        # Write each element of the list to a new line\n",
    "        for item in numgt:\n",
    "            file.write(f\"{item}\\n\")\n",
    "            \n",
    "write(train_target, train_number_type, train_number_gt, 'train')\n",
    "write(dev_target, dev_number_type, dev_number_gt, 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ad1c7-95cf-453b-bd52-8aabcd340289",
   "metadata": {},
   "source": [
    "## 2. Data processing and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32192b15-fbcb-4c89-969b-8453a7f743e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c88dad4c-e89c-4057-b074-e9c411f305e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MASK_TOKEN = tokenizer.mask_token\n",
    "MASK_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b57b024b-e23a-4aad-b5de-8519d3f63c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sample, replace_token='mask', task='train'):\n",
    "    '''\n",
    "    teacher forcing only during training, hence reasoning prompt would be prepended only to the train samples\n",
    "    '''\n",
    "    \n",
    "    news = sample['news']\n",
    "    masked_headline = sample['masked headline']\n",
    "    calculation = sample['calculation']\n",
    "    ans = str(sample['ans'])\n",
    "    \n",
    "    if replace_token=='mask':\n",
    "        replace_token=MASK_TOKEN\n",
    "    else:\n",
    "        replace_token=ans\n",
    "        \n",
    "    if task=='train':\n",
    "        input_prompt = \"Given the news article, perform \" + calculation + \" to fill in the mask token : \" + \"\\n\" + news + \" \" + masked_headline.replace('____', replace_token)\n",
    "    else:\n",
    "        input_prompt = \"Given the news article, fill in the mask token : \" + \"\\n\" + news + \" \" + masked_headline.replace('____', replace_token)\n",
    "    \n",
    "    if \"copy\" in calculation.lower():\n",
    "        reasoning = 1\n",
    "    else:\n",
    "        reasoning = 0\n",
    "    \n",
    "    return {\"input_prompt\":input_prompt, \"ans\": ans, \"reasoning\":reasoning}\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return tokenizer.encode_plus(sentence,\n",
    "                                 max_length=512,\n",
    "                                 padding='max_length',\n",
    "                                 truncation=True,\n",
    "                                 return_tensors='pt',\n",
    "                                 return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff742aab-5107-4d76-947e-e488cdd92a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21157/21157 [00:00<00:00, 263657.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2572/2572 [00:00<00:00, 265904.61it/s]\n"
     ]
    }
   ],
   "source": [
    "train_processed = [process_data(sample, replace_token='ans') for sample in tqdm(train_data)]\n",
    "dev_processed = [process_data(sample, replace_token='ans', task='dev') for sample in tqdm(dev_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47a96065-c849-4b7e-aa38-394e98a8f0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': \"Given the news article, perform Copy(5) to fill in the mask token : \\n(Jul 27, 2010  9:52 AM CDT) Previous reports estimated that Chelsea Clinton’s wedding would cost a cool $2 million, but the real number is probably more like $3 million to $5 million. Wedding experts run down the costs for the New York Daily News, from $600,000 air-conditioned tents to the $150-a-pop invitations and $100 place settings for each of the 500 guests. At the more conservative $3 million estimate, the total cost comes to $6,000 per guest. But it’s not all designer dresses, fancy food, and $15,000 port-a-potties (yes, $15,000 for  outhouses  that are much nicer than your bathroom at home—TMZ has pictures). Because of the high-profile nature of the event, security will probably run at least $200,000 (even though the White House confirms President Obama won't attend)—or more if they opt to shut down air space or pay police to monitor traffic. Overcome with Clinton wedding fever? Click here for 10 things that would make Chelsea's big day even more awesome. Chelsea's Wedding Tab: $3M to $5M\",\n",
       " 'ans': '5',\n",
       " 'reasoning': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3cf7a74-ea4c-418d-981a-f73a4d98a78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': \"Given the news article, fill in the mask token : \\n(Oct 16, 2014  3:02 AM CDT) Tristen Kurilla, the Pennsylvania 10-year-old who confessed to killing a 90-year-old woman over the weekend, is still in an adult prison and for now, his family thinks that could be the best place for him. His attorney withdrew a bail request yesterday saying the  family just doesn't feel comfortable for numerous reasons,  including concern for the family of Helen Novak, whom Tristen allegedly killed with a walking stick, reports WBRE. The lawyer also cited the family's worries over  the supervision, of work, of everything.  The district attorney says the boy is not in the  general population. He is not in solitary confinement and he is not in isolation. He is in a cell which I believe is next to the infirmary where he can come and go from the cell to the infirmary.  The nearest juvenile detention facility is 80 miles away. The boy has been provided with coloring books and other forms of entertainment, and while he understands where he is, he's acting like a normal 10-year-old and talks about his prison outfit  being a Halloween costume,  says his lawyer. Tristen has been charged as an adult with homicide, but his lawyer plans to have the case transferred to juvenile court, a move the chief counsel of the Juvenile Law Center in Philadelphia tells the AP is a  no-brainer.  The fifth-grader is  a little boy,  she says.  It's a horrible tragedy, but it's shocking that he suddenly turned into an adult because of conduct that he engaged in.  She says he is among the youngest people ever charged with homicide in Pennsylvania. The youngest on record was 9 years old. Murder Suspect, 10, Will Stay in Adult Jail\",\n",
       " 'ans': '10',\n",
       " 'reasoning': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_processed[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b224450c-fa74-40ab-9a66-9c54c8779dd5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21157/21157 [00:17<00:00, 1235.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2572/2572 [00:01<00:00, 1383.59it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = [tokenize(sample['input_prompt']) for sample in tqdm(train_processed)]\n",
    "dev_tokenized = [tokenize(sample['input_prompt']) for sample in tqdm(dev_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0210516c-2f6f-4ad2-b33d-38337fdddf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21157/21157 [00:00<00:00, 21170.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2572/2572 [00:00<00:00, 19548.02it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get input ids for all the masked token\n",
    "'''\n",
    "masked_train_input_ids = [tokenizer(psamples['ans'])['input_ids'][1:-1] for psamples in tqdm(train_processed)]\n",
    "masked_dev_input_ids = [tokenizer(psamples['ans'])['input_ids'][1:-1] for psamples in tqdm(dev_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eae15585-d3e9-43d8-9174-13352f11dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MLM labels\n",
    "'''\n",
    "train_ans_label = [inp['input_ids'].clone().detach() for inp in train_tokenized]\n",
    "dev_ans_label = [inp['input_ids'].clone().detach() for inp in dev_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a4c3a0b-f078-4b14-9934-bcfdebd78c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_index_of_continuous_subset(lst, subset):\n",
    "    subset_length = len(subset)\n",
    "    \n",
    "    for i in range(len(lst) - subset_length, -1, -1):\n",
    "        if lst[i:i + subset_length] == subset:\n",
    "            return i\n",
    "    \n",
    "    # Return -1 if the subset is not found\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d967ae4-f0a9-4df5-bb37-54eacc406ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_tok_id = tokenizer(MASK_TOKEN)['input_ids'][1:-1][0]\n",
    "mask_tok_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f194ad5-d99e-414f-9968-966d4d5ae12c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_with_mask_ids(tokenized_seq, to_mask_ids):\n",
    "    \n",
    "    all_masked_inds = []\n",
    "    \n",
    "    for i, token in tqdm(enumerate(tokenized_seq)):\n",
    "        masked_inds = []\n",
    "        \n",
    "        total_ls = token['input_ids'][0].tolist()\n",
    "        sublist = to_mask_ids[i]\n",
    "    \n",
    "        last_index_of_subset = find_last_index_of_continuous_subset(total_ls, sublist)\n",
    "        masked_inds = list(range(last_index_of_subset, last_index_of_subset+len(sublist)))\n",
    "        \n",
    "        all_masked_inds.append(masked_inds)\n",
    "        \n",
    "        for ind in masked_inds:\n",
    "            tokenized_seq[i]['input_ids'][0][ind] = mask_tok_id\n",
    "        \n",
    "    return tokenized_seq, all_masked_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59a15753-5ace-42b3-a83a-ad8c365c94e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21157it [00:01, 13482.64it/s]\n",
      "2572it [00:00, 12309.54it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model inputs\n",
    "'''\n",
    "train_inp, train_masked_inds = replace_with_mask_ids(train_tokenized, masked_train_input_ids)\n",
    "dev_inp, dev_masked_inds = replace_with_mask_ids(dev_tokenized, masked_dev_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91fe1d00-3cce-48a4-a377-2c9442fa5086",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21157/21157 [00:00<00:00, 1388519.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2572/2572 [00:00<00:00, 2276376.85it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Reasoning labels\n",
    "'''\n",
    "train_reasoning_label = [sample['reasoning'] for sample in tqdm(train_processed)]\n",
    "dev_reasoning_label = [sample['reasoning'] for sample in tqdm(dev_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e010250-b93c-47ed-9dc3-6182222f81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_ids, attn_masks, mask_label, reasoning_label, masked_inds):\n",
    "        'Initialization'\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attn_masks\n",
    "        self.mask_labels = mask_label\n",
    "        self.reasoning_label = reasoning_label\n",
    "        self.masked_inds = masked_inds\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.mask_labels[idx], self.reasoning_label[idx], self.masked_inds[idx]\n",
    "\n",
    "def extract_ids_and_masks(inputs):\n",
    "    \n",
    "    input_ids = torch.stack([item['input_ids'] for item in inputs]).squeeze(1)\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in inputs]).squeeze(1)\n",
    "    \n",
    "    return input_ids.to(device), attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "348cd356-5d46-4886-93bc-06b974597c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inp_ids, train_attn_masks = extract_ids_and_masks(train_inp)\n",
    "dev_inp_ids, dev_attn_masks = extract_ids_and_masks(dev_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "478eed77-eacd-4284-8016-947bfb5f208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': shuffle\n",
    "}\n",
    "\n",
    "train_set = NumDataset(train_inp_ids, train_attn_masks, train_ans_label, train_reasoning_label, train_masked_inds)\n",
    "training_generator = torch.utils.data.DataLoader(train_set, **params)\n",
    "\n",
    "dev_set = NumDataset(dev_inp_ids, dev_attn_masks, dev_ans_label, train_reasoning_label, dev_masked_inds)\n",
    "dev_generator = torch.utils.data.DataLoader(dev_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3bcf0-9c57-413f-963e-509afa9927f6",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d14f66a-d917-4ede-8f1b-44b70cee5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d57deb98-761b-4e13-b4d2-b3e7c79dda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryReasoningClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : copy, not_copy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super(BinaryReasoningClassifier, self).__init__()\n",
    "        # custom layer for binary classification\n",
    "        self.binary_classification_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.01),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.binary_classification_head(x)\n",
    "\n",
    "class MaskedPredictor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super(MaskedPredictor, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "    \n",
    "class NumGenModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(NumGenModel, self).__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        self.classifier = BinaryReasoningClassifier(hidden=self.hidden_size)\n",
    "        self.mask_lm = MaskedPredictor(d_model=self.hidden_size, vocab_size=self.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # Get the BERT model outputs\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "        # Extract the pooled output (CLS token) for binary classification\n",
    "        pooled_output = outputs.pooler_output\n",
    "        last_hidden = output.last_hidden_state\n",
    "        \n",
    "        reasoning_preds = self.classifier(pooled_output)\n",
    "        masked_token_preds = self.mask_lm(last_hidden)\n",
    "        \n",
    "        return reasoning_preds, masked_token_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c17b21d5-abd9-4713-9e38-292fbf619de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "            \n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        train_dataloader, \n",
    "        dev_dataloader, \n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999),\n",
    "        warmup_steps=10000,\n",
    "        log_freq=10\n",
    "        ):\n",
    "\n",
    "        self.model = model\n",
    "        self.train_data = train_dataloader\n",
    "        self.dev_data = dev_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(\n",
    "            self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.dev_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "        \n",
    "        mode = \"train\" if train else \"test\"\n",
    "\n",
    "        # progress bar\n",
    "        data_iter = tqdm.tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "            total=len(data_loader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "\n",
    "        for i, batch in data_iter:\n",
    "\n",
    "            inp_ids, attention_mask, mask_label, reasoning_label, masked_inds = batch\n",
    "            \n",
    "            reasoning_pred, token_pred = self.model(input_ids=inp_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            reasoning_pred = reasoning_pred.argmax(axis=1)\n",
    "            \n",
    "            next_loss = self.criterion(reasoning_pred, reasoning_label)\n",
    "            \n",
    "            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n",
    "            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n",
    "            masked_token_preds = token_pred[masked_inds]\n",
    "            mask_label = mask_label[masked_inds]\n",
    "            \n",
    "            mask_loss = self.criterion(masked_token_preds, mask_label)\n",
    "            # mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "            loss = next_loss + mask_loss\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "        print(\n",
    "            f\"EP{epoch}, {mode}: \\\n",
    "            avg_loss={avg_loss / len(data_iter)}, \\\n",
    "            total_acc={total_correct * 100.0 / total_element}\"\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc4cb938-627c-48e3-b86d-4a0e360578d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NumGenModel(\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ad1fe-b0ab-4312-8760-cf44ab094afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, training_generator, dev_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343dede-a639-49c0-9455-06c88fe41093",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bert_trainer.train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969aec0-63ab-4643-8136-48b9c311fa98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516567cf-f68c-43ef-b55b-68e10867cb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c692d0-0296-47aa-9456-c3a93471a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalUnderstandingModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels_head1=1, num_labels_head2=2):\n",
    "        super(NumericalUnderstandingModel, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Freeze the pre-trained transformer parameters\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Classification head 1: Fill in the mask token\n",
    "        self.head1 = nn.Linear(self.transformer.config.hidden_size, num_labels_head1)\n",
    "\n",
    "        # Classification head 2: Copy vs. Operation\n",
    "        self.head2 = nn.Linear(self.transformer.config.hidden_size, num_labels_head2)\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Tokenize input text and get transformer outputs\n",
    "        input_ids = self.tokenizer(input_text, return_tensors='pt')['input_ids']\n",
    "        outputs = self.transformer(input_ids)\n",
    "\n",
    "        # Get the representation of the [MASK] token (or other relevant token)\n",
    "        mask_token_index = input_text.index('[MASK]')  # Replace with the actual token used\n",
    "        mask_token_representation = outputs.last_hidden_state[:, mask_token_index, :]\n",
    "\n",
    "        # Classification head 1: Fill in the mask token\n",
    "        head1_output = self.head1(mask_token_representation)\n",
    "\n",
    "        # Classification head 2: Copy vs. Operation\n",
    "        pooled_output = outputs.pooler_output\n",
    "        head2_output = self.head2(pooled_output)\n",
    "\n",
    "        return head1_output, head2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12bf1e-5ecf-463e-8cb9-b64722af8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define your numerical understanding dataset and DataLoader\n",
    "class NumericalDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.texts[idx], 'label': self.labels[idx]}\n",
    "\n",
    "numerical_texts = [...]  # Replace with your numerical dataset texts\n",
    "numerical_labels = [...]  # Replace with your numerical dataset labels\n",
    "numerical_dataset = NumericalDataset(numerical_texts, numerical_labels)\n",
    "numerical_dataloader = DataLoader(numerical_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define your headline generation dataset and DataLoader\n",
    "class HeadlineDataset(Dataset):\n",
    "    def __init__(self, texts, headlines):\n",
    "        self.texts = texts\n",
    "        self.headlines = headlines\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.texts[idx], 'headline': self.headlines[idx]}\n",
    "\n",
    "headline_texts = [...]  # Replace with your headline dataset texts\n",
    "headline_headlines = [...]  # Replace with your headline dataset headlines\n",
    "headline_dataset = HeadlineDataset(headline_texts, headline_headlines)\n",
    "headline_dataloader = DataLoader(headline_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Fine-tuning Phase 1: Numerical Understanding Task\n",
    "def fine_tune_numerical_task(model, dataloader, num_epochs=3, lr=1e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(dataloader, desc=f'Epoch {epoch + 1}'):\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "            labels = torch.tensor(batch['label'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch + 1}, Average Loss: {avg_loss}')\n",
    "\n",
    "# Load pre-trained encoder for numerical understanding\n",
    "encoder_model_name = 'bert-base-uncased'  # Replace with your desired encoder model\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\n",
    "encoder_model = AutoModelForSequenceClassification.from_pretrained(encoder_model_name, num_labels=2)\n",
    "\n",
    "# Fine-tune the encoder on the numerical understanding task\n",
    "fine_tune_numerical_task(encoder_model, numerical_dataloader)\n",
    "\n",
    "# Fine-tuning Phase 2: Headline Generation Task\n",
    "# Load pre-trained decoder for headline generation\n",
    "decoder_model_name = 't5-small'  # Replace with your desired decoder model\n",
    "decoder_model = AutoModelForCausalLM.from_pretrained(decoder_model_name)\n",
    "\n",
    "# Combine the fine-tuned encoder with the pre-trained decoder\n",
    "combined_model = nn.Sequential(encoder_model, decoder_model)\n",
    "\n",
    "# Fine-tune the combined model on the headline generation task\n",
    "# Note: Fine-tuning details may vary based on the decoder model; adjust as needed\n",
    "def fine_tune_headline_generation(model, dataloader, num_epochs=3, lr=1e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(dataloader, desc=f'Epoch {epoch + 1}'):\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "            labels = tokenizer(batch['headline'], return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch + 1}, Average Loss: {avg_loss}')\n",
    "\n",
    "# Fine-tune the combined model on the headline generation task\n",
    "fine_tune_headline_generation(combined_model, headline_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
